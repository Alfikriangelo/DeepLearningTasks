{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM238yaAiAGUzLcyI4FIDje",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alfikriangelo/DeepLearningTasks/blob/main/2ndWeekTask/Covertdataset_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
      ],
      "metadata": {
        "id": "jcBINqSSK9du"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"sample_data/compressed_data.csv\")"
      ],
      "metadata": {
        "id": "hAzO_7UfLJTx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hapus baris dengan NaN di target\n",
        "df = df.dropna(subset=['Cover_Type'])"
      ],
      "metadata": {
        "id": "f2rIpNrvTUiW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pisahkan fitur (X) dan target (y)\n",
        "X = df.drop(columns=[\"Cover_Type\"])\n",
        "y = df[\"Cover_Type\"] - 1  # Mengurangi 1 agar kelas mulai dari 0 (karena PyTorch membutuhkan label 0-based)"
      ],
      "metadata": {
        "id": "7ll_6lfLLLf3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagi dataset menjadi data latih dan uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "x4OiDtTaLM7a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi fitur\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "XkrXI3LfLRKv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konversi ke Tensor PyTorch\n",
        "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_torch = torch.tensor(y_test.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "zFeNvWlGLSlX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisi Model PyTorch\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Azojiyv9LjbG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi model\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 128\n",
        "num_classes = len(np.unique(y))\n",
        "model_torch = MLPModel(input_size, hidden_size, num_classes)"
      ],
      "metadata": {
        "id": "2767tb2ELnSE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisi loss function dan optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_torch.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "eYDC1YKgLpLb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping manual PyTorch\n",
        "early_stop_patience = 5\n",
        "best_loss = float(\"inf\")\n",
        "patience = 0"
      ],
      "metadata": {
        "id": "BIPkQxdkLqsD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model PyTorch\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_torch(X_train_torch)\n",
        "    loss = criterion(outputs, y_train_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validasi\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model_torch(X_test_torch)\n",
        "        val_loss = criterion(val_outputs, y_test_torch)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss.item() < best_loss:\n",
        "        best_loss = val_loss.item()\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience += 1\n",
        "    if patience >= early_stop_patience:\n",
        "        print(\"Early stopping!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_toqJ0GrLsKT",
        "outputId": "4a7504d2-6759-4471-fe65-324ffe5461e0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.020599126815796, Val Loss: 1.977095127105713\n",
            "Epoch 2, Loss: 1.9768424034118652, Val Loss: 1.9345585107803345\n",
            "Epoch 3, Loss: 1.934304118156433, Val Loss: 1.8932808637619019\n",
            "Epoch 4, Loss: 1.893025517463684, Val Loss: 1.853272557258606\n",
            "Epoch 5, Loss: 1.8530049324035645, Val Loss: 1.8145248889923096\n",
            "Epoch 6, Loss: 1.8142304420471191, Val Loss: 1.7770183086395264\n",
            "Epoch 7, Loss: 1.7766848802566528, Val Loss: 1.7407276630401611\n",
            "Epoch 8, Loss: 1.740343451499939, Val Loss: 1.7056175470352173\n",
            "Epoch 9, Loss: 1.7051739692687988, Val Loss: 1.671643853187561\n",
            "Epoch 10, Loss: 1.6711351871490479, Val Loss: 1.6387608051300049\n",
            "Epoch 11, Loss: 1.6381800174713135, Val Loss: 1.606918215751648\n",
            "Epoch 12, Loss: 1.606260061264038, Val Loss: 1.5760653018951416\n",
            "Epoch 13, Loss: 1.5753259658813477, Val Loss: 1.5461527109146118\n",
            "Epoch 14, Loss: 1.5453307628631592, Val Loss: 1.5171350240707397\n",
            "Epoch 15, Loss: 1.5162297487258911, Val Loss: 1.4889711141586304\n",
            "Epoch 16, Loss: 1.487981915473938, Val Loss: 1.461623191833496\n",
            "Epoch 17, Loss: 1.460548758506775, Val Loss: 1.4350565671920776\n",
            "Epoch 18, Loss: 1.4338959455490112, Val Loss: 1.4092388153076172\n",
            "Epoch 19, Loss: 1.407993197441101, Val Loss: 1.3841415643692017\n",
            "Epoch 20, Loss: 1.3828130960464478, Val Loss: 1.3597427606582642\n",
            "Epoch 21, Loss: 1.3583323955535889, Val Loss: 1.3360216617584229\n",
            "Epoch 22, Loss: 1.3345295190811157, Val Loss: 1.31295907497406\n",
            "Epoch 23, Loss: 1.3113874197006226, Val Loss: 1.290541648864746\n",
            "Epoch 24, Loss: 1.2888925075531006, Val Loss: 1.268757700920105\n",
            "Epoch 25, Loss: 1.2670331001281738, Val Loss: 1.2475981712341309\n",
            "Epoch 26, Loss: 1.2458014488220215, Val Loss: 1.2270554304122925\n",
            "Epoch 27, Loss: 1.2251911163330078, Val Loss: 1.2071243524551392\n",
            "Epoch 28, Loss: 1.2051963806152344, Val Loss: 1.1878024339675903\n",
            "Epoch 29, Loss: 1.1858128309249878, Val Loss: 1.16908597946167\n",
            "Epoch 30, Loss: 1.1670386791229248, Val Loss: 1.1509720087051392\n",
            "Epoch 31, Loss: 1.1488711833953857, Val Loss: 1.1334583759307861\n",
            "Epoch 32, Loss: 1.1313072443008423, Val Loss: 1.1165412664413452\n",
            "Epoch 33, Loss: 1.114343285560608, Val Loss: 1.1002169847488403\n",
            "Epoch 34, Loss: 1.0979760885238647, Val Loss: 1.084481954574585\n",
            "Epoch 35, Loss: 1.082201361656189, Val Loss: 1.069332242012024\n",
            "Epoch 36, Loss: 1.0670139789581299, Val Loss: 1.054760217666626\n",
            "Epoch 37, Loss: 1.0524077415466309, Val Loss: 1.040757179260254\n",
            "Epoch 38, Loss: 1.0383741855621338, Val Loss: 1.0273154973983765\n",
            "Epoch 39, Loss: 1.024904489517212, Val Loss: 1.0144239664077759\n",
            "Epoch 40, Loss: 1.0119872093200684, Val Loss: 1.0020709037780762\n",
            "Epoch 41, Loss: 0.9996106624603271, Val Loss: 0.9902416467666626\n",
            "Epoch 42, Loss: 0.9877614974975586, Val Loss: 0.9789218306541443\n",
            "Epoch 43, Loss: 0.9764244556427002, Val Loss: 0.9680957198143005\n",
            "Epoch 44, Loss: 0.9655833840370178, Val Loss: 0.957745373249054\n",
            "Epoch 45, Loss: 0.9552215933799744, Val Loss: 0.9478529691696167\n",
            "Epoch 46, Loss: 0.9453206062316895, Val Loss: 0.9384007453918457\n",
            "Epoch 47, Loss: 0.9358614683151245, Val Loss: 0.9293697476387024\n",
            "Epoch 48, Loss: 0.9268251657485962, Val Loss: 0.9207421541213989\n",
            "Epoch 49, Loss: 0.9181938171386719, Val Loss: 0.9125014543533325\n",
            "Epoch 50, Loss: 0.9099501371383667, Val Loss: 0.9046288728713989\n",
            "Epoch 51, Loss: 0.9020760655403137, Val Loss: 0.8971084952354431\n",
            "Epoch 52, Loss: 0.8945554494857788, Val Loss: 0.8899251818656921\n",
            "Epoch 53, Loss: 0.8873720169067383, Val Loss: 0.8830627799034119\n",
            "Epoch 54, Loss: 0.880510151386261, Val Loss: 0.876507043838501\n",
            "Epoch 55, Loss: 0.8739553093910217, Val Loss: 0.8702458143234253\n",
            "Epoch 56, Loss: 0.8676947355270386, Val Loss: 0.8642691969871521\n",
            "Epoch 57, Loss: 0.861717939376831, Val Loss: 0.8585673570632935\n",
            "Epoch 58, Loss: 0.8560143113136292, Val Loss: 0.8531304597854614\n",
            "Epoch 59, Loss: 0.8505737781524658, Val Loss: 0.8479490280151367\n",
            "Epoch 60, Loss: 0.845386803150177, Val Loss: 0.8430124521255493\n",
            "Epoch 61, Loss: 0.8404422998428345, Val Loss: 0.8383091688156128\n",
            "Epoch 62, Loss: 0.8357279300689697, Val Loss: 0.8338253498077393\n",
            "Epoch 63, Loss: 0.8312307000160217, Val Loss: 0.8295466899871826\n",
            "Epoch 64, Loss: 0.8269374370574951, Val Loss: 0.8254597187042236\n",
            "Epoch 65, Loss: 0.8228354454040527, Val Loss: 0.8215529918670654\n",
            "Epoch 66, Loss: 0.8189133405685425, Val Loss: 0.8178147077560425\n",
            "Epoch 67, Loss: 0.8151611089706421, Val Loss: 0.8142353296279907\n",
            "Epoch 68, Loss: 0.8115680813789368, Val Loss: 0.8108044862747192\n",
            "Epoch 69, Loss: 0.8081240653991699, Val Loss: 0.8075130581855774\n",
            "Epoch 70, Loss: 0.8048201203346252, Val Loss: 0.8043515682220459\n",
            "Epoch 71, Loss: 0.8016489744186401, Val Loss: 0.8013127446174622\n",
            "Epoch 72, Loss: 0.798607349395752, Val Loss: 0.7983890771865845\n",
            "Epoch 73, Loss: 0.7956861853599548, Val Loss: 0.7955734133720398\n",
            "Epoch 74, Loss: 0.792874276638031, Val Loss: 0.7928590774536133\n",
            "Epoch 75, Loss: 0.7901644110679626, Val Loss: 0.7902413606643677\n",
            "Epoch 76, Loss: 0.7875505089759827, Val Loss: 0.7877152562141418\n",
            "Epoch 77, Loss: 0.7850272059440613, Val Loss: 0.7852756381034851\n",
            "Epoch 78, Loss: 0.7825897932052612, Val Loss: 0.782918393611908\n",
            "Epoch 79, Loss: 0.7802340388298035, Val Loss: 0.7806398272514343\n",
            "Epoch 80, Loss: 0.777955949306488, Val Loss: 0.7784363031387329\n",
            "Epoch 81, Loss: 0.7757514119148254, Val Loss: 0.7763041853904724\n",
            "Epoch 82, Loss: 0.7736167311668396, Val Loss: 0.7742394804954529\n",
            "Epoch 83, Loss: 0.7715485692024231, Val Loss: 0.7722383737564087\n",
            "Epoch 84, Loss: 0.7695426940917969, Val Loss: 0.7702962160110474\n",
            "Epoch 85, Loss: 0.7675949931144714, Val Loss: 0.768408477306366\n",
            "Epoch 86, Loss: 0.7657013535499573, Val Loss: 0.7665712833404541\n",
            "Epoch 87, Loss: 0.7638579607009888, Val Loss: 0.7647800445556641\n",
            "Epoch 88, Loss: 0.7620609402656555, Val Loss: 0.763031542301178\n",
            "Epoch 89, Loss: 0.7603066563606262, Val Loss: 0.7613224983215332\n",
            "Epoch 90, Loss: 0.758592426776886, Val Loss: 0.759650468826294\n",
            "Epoch 91, Loss: 0.7569156885147095, Val Loss: 0.7580133676528931\n",
            "Epoch 92, Loss: 0.7552743554115295, Val Loss: 0.7564091682434082\n",
            "Epoch 93, Loss: 0.7536670565605164, Val Loss: 0.7548370957374573\n",
            "Epoch 94, Loss: 0.752092182636261, Val Loss: 0.7532967925071716\n",
            "Epoch 95, Loss: 0.7505488991737366, Val Loss: 0.7517871260643005\n",
            "Epoch 96, Loss: 0.7490362524986267, Val Loss: 0.7503073215484619\n",
            "Epoch 97, Loss: 0.7475531697273254, Val Loss: 0.7488566040992737\n",
            "Epoch 98, Loss: 0.7460988163948059, Val Loss: 0.7474340200424194\n",
            "Epoch 99, Loss: 0.7446721792221069, Val Loss: 0.7460389137268066\n",
            "Epoch 100, Loss: 0.7432723641395569, Val Loss: 0.7446696758270264\n",
            "Epoch 101, Loss: 0.7418982982635498, Val Loss: 0.7433257699012756\n",
            "Epoch 102, Loss: 0.7405486702919006, Val Loss: 0.7420056462287903\n",
            "Epoch 103, Loss: 0.7392223477363586, Val Loss: 0.74070805311203\n",
            "Epoch 104, Loss: 0.7379180192947388, Val Loss: 0.7394319772720337\n",
            "Epoch 105, Loss: 0.7366345524787903, Val Loss: 0.7381761074066162\n",
            "Epoch 106, Loss: 0.7353708744049072, Val Loss: 0.7369394302368164\n",
            "Epoch 107, Loss: 0.7341259121894836, Val Loss: 0.7357211112976074\n",
            "Epoch 108, Loss: 0.7328989505767822, Val Loss: 0.7345203161239624\n",
            "Epoch 109, Loss: 0.7316892743110657, Val Loss: 0.7333362102508545\n",
            "Epoch 110, Loss: 0.7304961085319519, Val Loss: 0.7321681380271912\n",
            "Epoch 111, Loss: 0.7293192744255066, Val Loss: 0.731015682220459\n",
            "Epoch 112, Loss: 0.7281581163406372, Val Loss: 0.7298785448074341\n",
            "Epoch 113, Loss: 0.7270122766494751, Val Loss: 0.7287561893463135\n",
            "Epoch 114, Loss: 0.7258810997009277, Val Loss: 0.7276475429534912\n",
            "Epoch 115, Loss: 0.7247641086578369, Val Loss: 0.7265523672103882\n",
            "Epoch 116, Loss: 0.7236611843109131, Val Loss: 0.7254698276519775\n",
            "Epoch 117, Loss: 0.7225715517997742, Val Loss: 0.7243999242782593\n",
            "Epoch 118, Loss: 0.7214949727058411, Val Loss: 0.7233418226242065\n",
            "Epoch 119, Loss: 0.7204309701919556, Val Loss: 0.7222951650619507\n",
            "Epoch 120, Loss: 0.7193793654441833, Val Loss: 0.7212598323822021\n",
            "Epoch 121, Loss: 0.7183395624160767, Val Loss: 0.7202351689338684\n",
            "Epoch 122, Loss: 0.7173111438751221, Val Loss: 0.7192209959030151\n",
            "Epoch 123, Loss: 0.7162941694259644, Val Loss: 0.7182179093360901\n",
            "Epoch 124, Loss: 0.7152881622314453, Val Loss: 0.7172256708145142\n",
            "Epoch 125, Loss: 0.7142931818962097, Val Loss: 0.7162438631057739\n",
            "Epoch 126, Loss: 0.7133088707923889, Val Loss: 0.7152721285820007\n",
            "Epoch 127, Loss: 0.7123350501060486, Val Loss: 0.714310348033905\n",
            "Epoch 128, Loss: 0.7113713622093201, Val Loss: 0.7133585810661316\n",
            "Epoch 129, Loss: 0.7104178071022034, Val Loss: 0.7124165296554565\n",
            "Epoch 130, Loss: 0.7094738483428955, Val Loss: 0.7114842534065247\n",
            "Epoch 131, Loss: 0.7085395455360413, Val Loss: 0.7105616927146912\n",
            "Epoch 132, Loss: 0.7076147198677063, Val Loss: 0.7096486687660217\n",
            "Epoch 133, Loss: 0.706699013710022, Val Loss: 0.7087445259094238\n",
            "Epoch 134, Loss: 0.7057923674583435, Val Loss: 0.7078491449356079\n",
            "Epoch 135, Loss: 0.7048940658569336, Val Loss: 0.706962525844574\n",
            "Epoch 136, Loss: 0.7040041089057922, Val Loss: 0.7060840129852295\n",
            "Epoch 137, Loss: 0.7031221985816956, Val Loss: 0.7052134871482849\n",
            "Epoch 138, Loss: 0.7022482752799988, Val Loss: 0.704351007938385\n",
            "Epoch 139, Loss: 0.7013821005821228, Val Loss: 0.7034962773323059\n",
            "Epoch 140, Loss: 0.7005237340927124, Val Loss: 0.7026488184928894\n",
            "Epoch 141, Loss: 0.6996729969978333, Val Loss: 0.7018086314201355\n",
            "Epoch 142, Loss: 0.6988295316696167, Val Loss: 0.7009755969047546\n",
            "Epoch 143, Loss: 0.6979931592941284, Val Loss: 0.7001495957374573\n",
            "Epoch 144, Loss: 0.6971638202667236, Val Loss: 0.6993302702903748\n",
            "Epoch 145, Loss: 0.6963413953781128, Val Loss: 0.6985175609588623\n",
            "Epoch 146, Loss: 0.6955256462097168, Val Loss: 0.6977113485336304\n",
            "Epoch 147, Loss: 0.6947166323661804, Val Loss: 0.6969113945960999\n",
            "Epoch 148, Loss: 0.693913996219635, Val Loss: 0.6961172819137573\n",
            "Epoch 149, Loss: 0.693117618560791, Val Loss: 0.6953291893005371\n",
            "Epoch 150, Loss: 0.6923277378082275, Val Loss: 0.694547176361084\n",
            "Epoch 151, Loss: 0.6915444135665894, Val Loss: 0.6937705278396606\n",
            "Epoch 152, Loss: 0.6907669901847839, Val Loss: 0.6929993629455566\n",
            "Epoch 153, Loss: 0.6899955868721008, Val Loss: 0.6922337412834167\n",
            "Epoch 154, Loss: 0.6892301440238953, Val Loss: 0.6914738416671753\n",
            "Epoch 155, Loss: 0.6884705424308777, Val Loss: 0.6907191872596741\n",
            "Epoch 156, Loss: 0.6877166628837585, Val Loss: 0.6899703145027161\n",
            "Epoch 157, Loss: 0.6869683861732483, Val Loss: 0.6892271041870117\n",
            "Epoch 158, Loss: 0.6862260699272156, Val Loss: 0.6884896755218506\n",
            "Epoch 159, Loss: 0.6854891777038574, Val Loss: 0.6877578496932983\n",
            "Epoch 160, Loss: 0.6847583055496216, Val Loss: 0.6870314478874207\n",
            "Epoch 161, Loss: 0.6840330958366394, Val Loss: 0.6863107681274414\n",
            "Epoch 162, Loss: 0.6833134293556213, Val Loss: 0.6855957508087158\n",
            "Epoch 163, Loss: 0.6825994253158569, Val Loss: 0.6848862171173096\n",
            "Epoch 164, Loss: 0.6818908452987671, Val Loss: 0.6841819882392883\n",
            "Epoch 165, Loss: 0.6811879873275757, Val Loss: 0.6834832429885864\n",
            "Epoch 166, Loss: 0.6804903745651245, Val Loss: 0.6827899217605591\n",
            "Epoch 167, Loss: 0.6797983050346375, Val Loss: 0.682101845741272\n",
            "Epoch 168, Loss: 0.679111897945404, Val Loss: 0.6814191341400146\n",
            "Epoch 169, Loss: 0.67843097448349, Val Loss: 0.6807418465614319\n",
            "Epoch 170, Loss: 0.6777556538581848, Val Loss: 0.680070161819458\n",
            "Epoch 171, Loss: 0.6770856976509094, Val Loss: 0.6794039607048035\n",
            "Epoch 172, Loss: 0.6764211058616638, Val Loss: 0.6787427067756653\n",
            "Epoch 173, Loss: 0.6757619976997375, Val Loss: 0.6780866384506226\n",
            "Epoch 174, Loss: 0.675108015537262, Val Loss: 0.6774354577064514\n",
            "Epoch 175, Loss: 0.6744590401649475, Val Loss: 0.676789402961731\n",
            "Epoch 176, Loss: 0.673815131187439, Val Loss: 0.6761483550071716\n",
            "Epoch 177, Loss: 0.6731761693954468, Val Loss: 0.6755124926567078\n",
            "Epoch 178, Loss: 0.67254239320755, Val Loss: 0.674881100654602\n",
            "Epoch 179, Loss: 0.6719135046005249, Val Loss: 0.6742547750473022\n",
            "Epoch 180, Loss: 0.6712895035743713, Val Loss: 0.6736335754394531\n",
            "Epoch 181, Loss: 0.6706703901290894, Val Loss: 0.6730172038078308\n",
            "Epoch 182, Loss: 0.6700562238693237, Val Loss: 0.6724051237106323\n",
            "Epoch 183, Loss: 0.6694467663764954, Val Loss: 0.6717978119850159\n",
            "Epoch 184, Loss: 0.6688422560691833, Val Loss: 0.6711951494216919\n",
            "Epoch 185, Loss: 0.6682425737380981, Val Loss: 0.6705967783927917\n",
            "Epoch 186, Loss: 0.6676477193832397, Val Loss: 0.670003354549408\n",
            "Epoch 187, Loss: 0.6670573949813843, Val Loss: 0.6694144606590271\n",
            "Epoch 188, Loss: 0.6664717793464661, Val Loss: 0.6688302159309387\n",
            "Epoch 189, Loss: 0.6658908128738403, Val Loss: 0.6682509779930115\n",
            "Epoch 190, Loss: 0.6653141975402832, Val Loss: 0.667676568031311\n",
            "Epoch 191, Loss: 0.6647423505783081, Val Loss: 0.6671071648597717\n",
            "Epoch 192, Loss: 0.6641747355461121, Val Loss: 0.6665418744087219\n",
            "Epoch 193, Loss: 0.6636114716529846, Val Loss: 0.6659805178642273\n",
            "Epoch 194, Loss: 0.6630527377128601, Val Loss: 0.6654238700866699\n",
            "Epoch 195, Loss: 0.6624983549118042, Val Loss: 0.6648720502853394\n",
            "Epoch 196, Loss: 0.6619486212730408, Val Loss: 0.6643245220184326\n",
            "Epoch 197, Loss: 0.6614035367965698, Val Loss: 0.6637813448905945\n",
            "Epoch 198, Loss: 0.6608628630638123, Val Loss: 0.6632423996925354\n",
            "Epoch 199, Loss: 0.6603265404701233, Val Loss: 0.6627079248428345\n",
            "Epoch 200, Loss: 0.6597947478294373, Val Loss: 0.6621776223182678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi PyTorch\n",
        "y_pred_torch = torch.argmax(model_torch(X_test_torch), axis=1).numpy()\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred_torch))\n",
        "print(\"Presisi:\", precision_score(y_test, y_pred_torch, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_torch, average='weighted'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_torch, average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBxrYi8uLuQr",
        "outputId": "745fc702-5ec0-46fa-c6cf-2c161de61569"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.7278813799987952\n",
            "Presisi: 0.7237352966208289\n",
            "Recall: 0.7278813799987952\n",
            "F1 Score: 0.7114607853748885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Konversi ke TensorFlow\n",
        "X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.int32)"
      ],
      "metadata": {
        "id": "zv8sJNvvLw77"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisi Model TensorFlow\n",
        "model_tf = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(input_size,)),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbjV5GFDLyWc",
        "outputId": "02bb0857-9237-4770-d541-5fa43851d887"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kompilasi model\n",
        "model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "HgHl0MPpL2bs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping TensorFlow\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "BleCu2zGL4AP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model TensorFlow\n",
        "model_tf.fit(X_train_tf, y_train_tf, epochs=30, validation_data=(X_test_tf, y_test_tf), callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY_EDO_aL6NZ",
        "outputId": "0b4ff902-12b0-45a6-dba7-ec8b5dfb755b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3ms/step - accuracy: 0.7341 - loss: 0.6235 - val_accuracy: 0.7852 - val_loss: 0.5009\n",
            "Epoch 2/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.7902 - loss: 0.4882 - val_accuracy: 0.8025 - val_loss: 0.4681\n",
            "Epoch 3/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2ms/step - accuracy: 0.8077 - loss: 0.4540 - val_accuracy: 0.8189 - val_loss: 0.4369\n",
            "Epoch 4/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2ms/step - accuracy: 0.8186 - loss: 0.4326 - val_accuracy: 0.8217 - val_loss: 0.4292\n",
            "Epoch 5/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3ms/step - accuracy: 0.8253 - loss: 0.4175 - val_accuracy: 0.8310 - val_loss: 0.4141\n",
            "Epoch 6/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.8307 - loss: 0.4081 - val_accuracy: 0.8382 - val_loss: 0.3999\n",
            "Epoch 7/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 3ms/step - accuracy: 0.8349 - loss: 0.3992 - val_accuracy: 0.8395 - val_loss: 0.3963\n",
            "Epoch 8/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3ms/step - accuracy: 0.8381 - loss: 0.3911 - val_accuracy: 0.8385 - val_loss: 0.3937\n",
            "Epoch 9/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3ms/step - accuracy: 0.8410 - loss: 0.3853 - val_accuracy: 0.8390 - val_loss: 0.3906\n",
            "Epoch 10/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2ms/step - accuracy: 0.8431 - loss: 0.3795 - val_accuracy: 0.8456 - val_loss: 0.3777\n",
            "Epoch 11/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3ms/step - accuracy: 0.8448 - loss: 0.3754 - val_accuracy: 0.8439 - val_loss: 0.3834\n",
            "Epoch 12/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.8477 - loss: 0.3724 - val_accuracy: 0.8424 - val_loss: 0.3771\n",
            "Epoch 13/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.8485 - loss: 0.3706 - val_accuracy: 0.8504 - val_loss: 0.3716\n",
            "Epoch 14/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2ms/step - accuracy: 0.8497 - loss: 0.3674 - val_accuracy: 0.8460 - val_loss: 0.3699\n",
            "Epoch 15/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2ms/step - accuracy: 0.8514 - loss: 0.3633 - val_accuracy: 0.8501 - val_loss: 0.3677\n",
            "Epoch 16/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 3ms/step - accuracy: 0.8531 - loss: 0.3596 - val_accuracy: 0.8488 - val_loss: 0.3725\n",
            "Epoch 17/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 2ms/step - accuracy: 0.8537 - loss: 0.3575 - val_accuracy: 0.8487 - val_loss: 0.3737\n",
            "Epoch 18/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.8546 - loss: 0.3575 - val_accuracy: 0.8510 - val_loss: 0.3615\n",
            "Epoch 19/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - accuracy: 0.8568 - loss: 0.3526 - val_accuracy: 0.8506 - val_loss: 0.3655\n",
            "Epoch 20/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2ms/step - accuracy: 0.8564 - loss: 0.3508 - val_accuracy: 0.8553 - val_loss: 0.3573\n",
            "Epoch 21/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - accuracy: 0.8578 - loss: 0.3476 - val_accuracy: 0.8567 - val_loss: 0.3643\n",
            "Epoch 22/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.8564 - loss: 0.3509 - val_accuracy: 0.8600 - val_loss: 0.3500\n",
            "Epoch 23/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3ms/step - accuracy: 0.8583 - loss: 0.3472 - val_accuracy: 0.8500 - val_loss: 0.3638\n",
            "Epoch 24/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2ms/step - accuracy: 0.8587 - loss: 0.3452 - val_accuracy: 0.8494 - val_loss: 0.3704\n",
            "Epoch 25/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2ms/step - accuracy: 0.8598 - loss: 0.3442 - val_accuracy: 0.8551 - val_loss: 0.3580\n",
            "Epoch 26/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 3ms/step - accuracy: 0.8607 - loss: 0.3418 - val_accuracy: 0.8602 - val_loss: 0.3518\n",
            "Epoch 27/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 3ms/step - accuracy: 0.8617 - loss: 0.3400 - val_accuracy: 0.8547 - val_loss: 0.3538\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cb2a613b390>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi TensorFlow\n",
        "y_pred_tf = np.argmax(model_tf.predict(X_test_tf), axis=1)\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred_tf))\n",
        "print(\"Presisi:\", precision_score(y_test, y_pred_tf, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_tf, average='weighted'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_tf, average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSkgjpGFL7dG",
        "outputId": "fa526959-618d-4a0e-8430-0c9e695cbb00"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3632/3632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
            "Akurasi: 0.8599950087347142\n",
            "Presisi: 0.8606025732474466\n",
            "Recall: 0.8599950087347142\n",
            "F1 Score: 0.8581858088796285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📌 **Penjelasan Metrik Evaluasi**\n",
        "\n",
        "### **1. Akurasi**  \n",
        "**Pengertian**:  \n",
        "Akurasi mengukur seberapa sering model memprediksi dengan benar dari keseluruhan dataset.  \n",
        "\n",
        "**Persamaan**:  \n",
        "$$ Akurasi = \\frac{TP + TN}{TP + TN + FP + FN} $$  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Presisi**  \n",
        "**Pengertian**:  \n",
        "Presisi mengukur seberapa banyak prediksi positif yang benar dibandingkan dengan total prediksi positif.  \n",
        "\n",
        "**Persamaan**:  \n",
        "$$ Presisi = \\frac{TP}{TP + FP} $$  \n",
        "\n",
        "---\n",
        "\n",
        "### **3. Recall (Sensitivitas / True Positive Rate)**  \n",
        "**Pengertian**:  \n",
        "Recall mengukur seberapa banyak data positif yang berhasil diklasifikasikan dengan benar.  \n",
        "\n",
        "**Persamaan**:  \n",
        "$$ Recall = \\frac{TP}{TP + FN} $$  \n",
        "\n",
        "---\n",
        "\n",
        "### **4. F1 Score**  \n",
        "**Pengertian**:  \n",
        "F1 Score adalah rata-rata harmonik antara **Presisi** dan **Recall**.  \n",
        "\n",
        "**Persamaan**:  \n",
        "$$ F1 Score = 2 \\times \\frac{Presisi \\times Recall}{Presisi + Recall} $$  \n",
        "\n",
        "---\n",
        "\n",
        "## 🏆 **Kesimpulan dari Hasil Evaluasi**  \n",
        "\n",
        "| Model        | Akurasi | Presisi | Recall | F1 Score |\n",
        "|-------------|--------|---------|--------|----------|\n",
        "| **PyTorch**  | 0.728  | 0.724   | 0.728  | 0.711    |\n",
        "| **TensorFlow** | 0.860  | 0.861   | 0.860  | 0.858    |\n",
        "\n",
        "✅ **TensorFlow lebih akurat, lebih presisi, lebih sensitif terhadap data positif, dan lebih seimbang dalam prediksi.**  \n",
        "\n",
        "💡 **Semoga membantu! 🚀**\n"
      ],
      "metadata": {
        "id": "wiJ7QMfiaW9_"
      }
    }
  ]
}